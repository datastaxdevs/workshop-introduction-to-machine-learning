{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 4: FP-Growth and DataStax Analytics\n",
    "------\n",
    "<img src=\"images/pixarMovies.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    "\n",
    "#### Dataset: https://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we trying to learn from this dataset? \n",
    "\n",
    "# QUESTION: Can FP-Growth be used to find which movies to recommend to our users?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import cassandra\n",
    "import pyspark\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint, randrange\n",
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import collect_set\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read .env file for connection params\n",
    "dotenv_file = find_dotenv('.env')\n",
    "load_dotenv(dotenv_file)\n",
    "astraUsername = os.environ[\"ASTRA_DB_CLIENT_ID\"]\n",
    "astraPassword = os.environ[\"ASTRA_DB_CLIENT_SECRET\"]\n",
    "astraSecureConnect = os.environ[\"ASTRA_DB_SECURE_BUNDLE_PATH\"]\n",
    "astraKeyspace = os.environ[\"ASTRA_DB_KEYSPACE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for pretty formatting for Spark DataFrames\n",
    "def showDF(df, limitRows =  5, truncate = True):\n",
    "    if(truncate):\n",
    "        pandas.set_option('display.max_colwidth', 50)\n",
    "    else:\n",
    "        pandas.set_option('display.max_colwidth', None)\n",
    "    pandas.set_option('display.max_rows', limitRows)\n",
    "    display(df.limit(limitRows).toPandas())\n",
    "    pandas.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to have nicer formatting of Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables and Loading Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dselogo.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "cloud_config = {\n",
    "    'secure_connect_bundle': '/home/jovyan/' + astraSecureConnect\n",
    "}\n",
    "auth_provider = PlainTextAuthProvider(username=astraUsername, password=astraPassword)\n",
    "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace(astraKeyspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table called `movies`. Our PRIMARY will be a unique key (movieid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"CREATE TABLE IF NOT EXISTS movies \\\n",
    "#                                    (movieid int, title text, genres text, \\\n",
    "#                                    PRIMARY KEY (movieid))\"\n",
    "# session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table called `movieRatings`. Our PRIMARY key will be a compositite key (userid, movieid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"CREATE TABLE IF NOT EXISTS movieratings \\\n",
    "#                                    (userid int, movieid int, rating float, timestamp text, \\\n",
    "#                                    PRIMARY KEY (userid, movieid))\"\n",
    "# session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movies\n",
    "\n",
    "* **Movieid**\n",
    "* **Title**\n",
    "* **Genres**\n",
    "\n",
    "## Movie Ratings Table\n",
    "### What do these of these 4 columns represent:\n",
    "\n",
    "* **UserId**\n",
    "* **MovieId**\n",
    "* **Rating**\n",
    "* **Timestamp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bttf3.jpg\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Apache Spark\n",
    "<img src=\"images/sparklogo.png\" width=\"150\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a spark session that is connected to cassandra. From there load each table into a Spark Dataframe and take a count of the number of rows in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('demo') \\\n",
    "    .master(\"local\") \\\n",
    "    .config( \\\n",
    "        \"spark.cassandra.connection.config.cloud.path\", \\\n",
    "        \"file:\" + '/home/jovyan/' + astraSecureConnect) \\\n",
    "    .config(\"spark.cassandra.auth.username\", astraUsername) \\\n",
    "    .config(\"spark.cassandra.auth.password\", astraPassword) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "movieDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"movieratings\", keyspace=astraKeyspace).load()\n",
    "\n",
    "print (\"Table Row Count: \")\n",
    "print (movieDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(movieDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This dataset is not in the format we need it to be. We need it to be more in a transaction format. Each user and the list of movies they have reviewed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the timestamp column since we will not be using that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newMovieDF = movieDF.drop('timestamp')\n",
    "showDF(newMovieDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we collect the set of movies for each user let's filter out any movies they rated less or equal to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newestMovies = newMovieDF.filter(\"rating > 3\")\n",
    "showDF(newestMovies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy the user id and create a collection set of all the movies they have rated and seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_user = newestMovies.groupBy('userid').agg(collect_set('movieid').alias('moviesRated'))\n",
    "group_user.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For FP-Growth the list needs to be a column named `items`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = group_user.withColumnRenamed(\"moviesRated\", \"items\")\n",
    "showDF(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('userid').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPGROWTH for  Recommendations\n",
    "#### Use Apache Spark MLlib with FPGrowth to find Recommendation \n",
    "#### https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html\n",
    "#### https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.fpm.FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.1, minConfidence=0.2)\n",
    "model = fpGrowth.fit(df)\n",
    "recommendDF=model.transform(df)\n",
    "recommendDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you have watched these movies `antecedent` then you will like this movie `consequent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display frequent itemsets.\n",
    "#model.freqItemsets.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "dfAssociation = model.associationRules\n",
    "\n",
    "dfAssociation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"If you like these movies: \")\n",
    "print(list(dfAssociation.select('antecedent').first()))\n",
    "print(\"Then you will like this movie:\")\n",
    "print(list(dfAssociation.select('consequent').first()))\n",
    "\n",
    "movieYoulike = list(dfAssociation.select('antecedent').first())\n",
    "movieToRecommend=list(dfAssociation.select('consequent').first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query database to get movie titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select title from movies WHERE movieid=\"\n",
    "query = query + str(movieYoulike[0][0])\n",
    "\n",
    "rows = session.execute(query)\n",
    "print(rows)\n",
    "\n",
    "for user_row in rows:\n",
    "    print (user_row.title)\n",
    "\n",
    "query = \"select title from movies WHERE movieid=\"\n",
    "query = query + str(movieYoulike[0][1])\n",
    "\n",
    "rows = session.execute(query)\n",
    "print(rows)\n",
    "\n",
    "for user_row in rows:\n",
    "    print (user_row.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then you will like this movie ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select title from movies WHERE movieid=\"\n",
    "query = query + str(movieToRecommend[0][0])\n",
    "\n",
    "rows = session.execute(query)\n",
    "print(rows)\n",
    "\n",
    "for user_row in rows:\n",
    "    print (user_row.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, col\n",
    "from operator import itemgetter\n",
    "\n",
    "def associate_movie_by_id(input_movie_id):\n",
    "    consequent_counts = dfAssociation\\\n",
    "        .filter(array_contains(col('antecedent'), input_movie_id))\\\n",
    "        .select('consequent')\\\n",
    "        .rdd\\\n",
    "        .flatMap(lambda x: x)\\\n",
    "        .flatMap(lambda x: x)\\\n",
    "        .groupBy(lambda x: x)\\\n",
    "        .map(lambda xy: (xy[0], len(xy[1])))\\\n",
    "        .collect()\n",
    "    if len(consequent_counts):\n",
    "        return sorted(consequent_counts, key=itemgetter(1), reverse=True)[0][0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associate_movie_by_id(input_movie_id=527)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
