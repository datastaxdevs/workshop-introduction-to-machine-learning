{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2: Naive Bayes and DataStax Analytics\n",
    "------\n",
    "<img src=\"images/drinkWine.jpeg\" width=\"300\" height=\"500\">\n",
    "\n",
    "\n",
    "#### Dataset: https://archive.ics.uci.edu/ml/datasets/Wine+Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we trying to learn from this dataset? \n",
    "\n",
    "# QUESTION:  Can Naive Bayes be used to classify a wineâ€™s rating score by its attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import cassandra\n",
    "import pyspark\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from random import randint, randrange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read .env file for connection params\n",
    "dotenv_file = find_dotenv('.env')\n",
    "load_dotenv(dotenv_file)\n",
    "astraUsername = os.environ[\"ASTRA_DB_CLIENT_ID\"]\n",
    "astraPassword = os.environ[\"ASTRA_DB_CLIENT_SECRET\"]\n",
    "astraSecureConnect = os.environ[\"ASTRA_DB_SECURE_BUNDLE_PATH\"]\n",
    "astraKeyspace = os.environ[\"ASTRA_DB_KEYSPACE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to have nicer formatting of Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for pretty formatting for Spark DataFrames\n",
    "def showDF(df, limitRows =  5, truncate = True):\n",
    "    if(truncate):\n",
    "        pandas.set_option('display.max_colwidth', 50)\n",
    "    else:\n",
    "        pandas.set_option('display.max_colwidth', None)\n",
    "    pandas.set_option('display.max_rows', limitRows)\n",
    "    display(df.limit(limitRows).toPandas())\n",
    "    pandas.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dselogo.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables and Loading Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "cloud_config = {\n",
    "    'secure_connect_bundle': '/home/jovyan/' + astraSecureConnect\n",
    "}\n",
    "auth_provider = PlainTextAuthProvider(username=astraUsername, password=astraPassword)\n",
    "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace(astraKeyspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table called `wines`. Our PRIMARY will be a unique key (wineid) we generate for each row.  This will have two datasets \"white\" and \"red\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"CREATE TABLE IF NOT EXISTS wines \\\n",
    "#                                    (wineid int, fixedacidity float, volatileacidity float, citricacid float, sugar float, \\\n",
    "#                                    chlorides float, freesulfur float, totalsulfur float, density float, ph float, \\\n",
    "#                                    sulphates float, alcohol float, quality float, \\\n",
    "#                                    PRIMARY KEY (wineid))\"\n",
    "# session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do these of these 12 columns represent: \n",
    "\n",
    "* **Fixed acidity**\n",
    "* **Volatile acidity**\n",
    "* **Citric Acid**\n",
    "* **Residual Sugar** \n",
    "* **Chlorides**\n",
    "* **Free sulfur dioxide**     \n",
    "* **Total sulfur dioxide**\n",
    "* **Density** \n",
    "* **pH**\n",
    "* **Sulphates**\n",
    "* **Alcohol**\n",
    "* **Quality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/whiteAndRed.jpeg\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Apache Cassandra & Apache Spark\n",
    "<img src=\"images/sparklogo.png\" width=\"150\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a spark session that is connected to the database. From there load each table into a Spark Dataframe and take a count of the number of rows in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('demo') \\\n",
    "    .master(\"local\") \\\n",
    "    .config( \\\n",
    "        \"spark.cassandra.connection.config.cloud.path\", \\\n",
    "        \"file:\" + '/home/jovyan/' + astraSecureConnect) \\\n",
    "    .config(\"spark.cassandra.auth.username\", astraUsername) \\\n",
    "    .config(\"spark.cassandra.auth.password\", astraPassword) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wineDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"wines\", keyspace=astraKeyspace).load()\n",
    "\n",
    "print (\"Table Wine Row Count: \")\n",
    "print (wineDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(wineDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's filter out only wines that have been rated 6.0 or higher and create a new dataframe with that information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine6DF = wineDF.filter(\"quality > 5\")\n",
    "showDF(wine6DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vector with all elements of the wine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['alcohol', 'chlorides', 'citricacid', 'density', 'fixedacidity', 'ph', 'freesulfur', 'sugar', 'sulphates', 'totalsulfur', 'volatileacidity'],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingData = assembler.transform(wine6DF)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"quality\", outputCol=\"label\", handleInvalid='keep')\n",
    "trainingData1 = labelIndexer.fit(trainingData).transform(trainingData)\n",
    "\n",
    "showDF(trainingData1)\n",
    "print(trainingData1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split up our dataset in to a training and test set. Will split 80/20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = trainingData1.randomSplit([0.8, 0.2], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "print (\"Train Dataframe Row Count: \")\n",
    "print (train.count())\n",
    "print (\"Test Dataframe Row Count: \")\n",
    "print (test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it's time to to use NaiveBayes. We will train the model, then use that model with out testing data to get our predictions. \n",
    "https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#naive-bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "#predictions.show()\n",
    "print (predictions.count())\n",
    "showDF(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(predictions.select(\"quality\", \"label\", \"prediction\", \"probability\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now use the MutliclassClassifciationEvaluator to evalute the accurancy of our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
